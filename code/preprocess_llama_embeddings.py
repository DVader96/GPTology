#------------------------------------------------#
# build dataframes from llama files generated by #
# mariano.                                       #
#------------------------------------------------#
import numpy as np
import pandas as pd

def concat_llama_embs(layer):
    emb_dir = '/scratch/gpfs/eham/247-encoding-updated/data/podcast2/777/pickles/llama-embeddings/'
    file_list = ['600', '1200', '1800', '2400', '3000', '3600', '4200', '4800', '5115']
    num_words = 5115
    emb_dim = 4096
    num_layers = 33

    all_emb = np.zeros((num_words, num_layers, emb_dim))
    all_words = []
    word_sum = 0
    emb_sum = 0
    for i in range(len(file_list)): 
        #breakpoint() 
        emb_f = np.load('{}podcast-embeddings_llama_7b_{}.npy'.format(emb_dir, file_list[i]))
        if i == len(file_list) - 1: emb_f = emb_f[:315]
        all_emb[i*600:int(file_list[i]),...] = emb_f
        word_f = np.load('{}podcast-words_llama_7b_{}.npy'.format(emb_dir, file_list[i]))
        all_words.extend(word_f)

    layer_emb = all_emb[:,layer,:]
    df = pd.DataFrame(all_words,  columns = ['word'])
    df = pd.concat([df, pd.DataFrame(pd.Series(list(layer_emb)),columns=['embeddings'])], axis=1)
    return df

if __name__ == '__main__':
    print('start')
    concat_llama_embs(0)
